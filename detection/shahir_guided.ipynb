{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations==0.4.6 in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from albumentations==0.4.6) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from albumentations==0.4.6) (1.13.0)\n",
      "Requirement already satisfied: imgaug>=0.4.0 in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from albumentations==0.4.6) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from albumentations==0.4.6) (6.0.1)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from albumentations==0.4.6) (4.9.0.80)\n",
      "Requirement already satisfied: six in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.16.0)\n",
      "Requirement already satisfied: Pillow in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (10.3.0)\n",
      "Requirement already satisfied: matplotlib in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.9.0)\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.23.2)\n",
      "Requirement already satisfied: imageio in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.34.1)\n",
      "Requirement already satisfied: Shapely in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.0.4)\n",
      "Requirement already satisfied: networkx>=2.8 in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (3.3)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2024.5.10)\n",
      "Requirement already satisfied: packaging>=21 in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (24.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/sazzad14/Documents/Stanford/CS-231N/portalcut/.venv/lib/python3.11/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.9.0.post0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install albumentations==0.4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic python and ML Libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# We will be reading images using OpenCV\n",
    "import cv2\n",
    "\n",
    "# matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# torchvision libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as torchtrans\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# helper libraries\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "# for image augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.models.detection as detection\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "# Send train=True for training transforms and False for val/test transforms\n",
    "def get_transform(train):\n",
    "    transform = [transforms.ToTensor()]\n",
    "    return transforms.Compose(transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kitti(torchvision.datasets.Kitti):\n",
    "    def __getitem__(self, index):\n",
    "        image, target = super().__getitem__(index)\n",
    "        # Convert target format from list of dicts to the correct dict format\n",
    "        labels = [['Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram', 'Misc', 'DontCare'].index(t['type']) for t in target]\n",
    "        boxes = [t['bbox'] for t in target]\n",
    "        \n",
    "        target = {'boxes': torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4), 'labels': torch.as_tensor(labels)}\n",
    "        return image, target\n",
    "\n",
    "dataset = Kitti(root='../data', transform=get_transform(train=True))\n",
    "dataset_test = Kitti(root='../data', transform=get_transform(train=False))\n",
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "\n",
    "# train test split\n",
    "test_split = 0.2\n",
    "tsize = int(len(dataset)*test_split)\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-tsize])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "  dataset,\n",
    "  batch_size=4,\n",
    "  shuffle=True,\n",
    "  num_workers=0,\n",
    "  collate_fn=utils.collate_fn,\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "  dataset_test,\n",
    "  batch_size=4,\n",
    "  shuffle=False,\n",
    "  num_workers=0,\n",
    "  collate_fn=utils.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_detection_model(num_classes):\n",
    "  # load a model pre-trained pre-trained on COCO\n",
    "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "  # get number of input features for the classifier\n",
    "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "  # replace the pre-trained head with a new one\n",
    "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2196, 0.2196, 0.2039,  ..., 0.0431, 0.0431, 0.0471],\n",
      "         [0.2196, 0.2196, 0.2157,  ..., 0.0471, 0.0471, 0.0431],\n",
      "         [0.2196, 0.2235, 0.2157,  ..., 0.0510, 0.0549, 0.0549],\n",
      "         ...,\n",
      "         [0.2902, 0.2588, 0.2549,  ..., 0.1176, 0.1255, 0.1294],\n",
      "         [0.2353, 0.2706, 0.2784,  ..., 0.1059, 0.1176, 0.1216],\n",
      "         [0.2392, 0.2980, 0.3373,  ..., 0.1059, 0.1176, 0.1216]],\n",
      "\n",
      "        [[0.3333, 0.3255, 0.3176,  ..., 0.0588, 0.0510, 0.0471],\n",
      "         [0.3294, 0.3216, 0.3137,  ..., 0.0745, 0.0588, 0.0510],\n",
      "         [0.3176, 0.3216, 0.3176,  ..., 0.0745, 0.0667, 0.0588],\n",
      "         ...,\n",
      "         [0.4157, 0.2824, 0.2510,  ..., 0.1647, 0.1725, 0.1647],\n",
      "         [0.3529, 0.2157, 0.2549,  ..., 0.1373, 0.1451, 0.1569],\n",
      "         [0.1686, 0.2392, 0.3176,  ..., 0.1176, 0.1294, 0.1451]],\n",
      "\n",
      "        [[0.4157, 0.4314, 0.4353,  ..., 0.0745, 0.0549, 0.0471],\n",
      "         [0.4118, 0.4314, 0.4431,  ..., 0.0667, 0.0588, 0.0588],\n",
      "         [0.4000, 0.4275, 0.4549,  ..., 0.0627, 0.0627, 0.0627],\n",
      "         ...,\n",
      "         [0.3020, 0.3059, 0.3961,  ..., 0.1686, 0.1608, 0.1686],\n",
      "         [0.2863, 0.3098, 0.4157,  ..., 0.1490, 0.1490, 0.1647],\n",
      "         [0.2627, 0.2549, 0.4627,  ..., 0.1176, 0.1098, 0.1373]]])\n",
      "<class 'dict'>\n",
      "{'boxes': tensor([[557.1800, 173.5500, 628.7500, 238.8100],\n",
      "        [ 32.0700,   0.0000, 520.3000, 375.0000],\n",
      "        [483.1300, 182.0800, 540.1200, 221.4100]]),\n",
      " 'labels': tensor([0, 2, 0])}\n",
      "data loader part\n",
      "4\n",
      "torch.Size([3, 375, 1242])\n",
      "<class 'tuple'>\n",
      "({'boxes': tensor([[ 334.2800,  180.6500,  490.0200,  297.4800],\n",
      "        [ 785.8500,  179.6700, 1028.8199,  340.7500],\n",
      "        [ 711.9800,  179.6500,  848.8200,  277.6200],\n",
      "        [ 445.9100,  131.6900,  539.7700,  228.3600],\n",
      "        [ 660.8200,  178.7200,  713.0900,  222.0300],\n",
      "        [ 620.9500,  171.5900,  649.3600,  200.4600],\n",
      "        [ 547.6600,  172.9300,  591.2600,  194.0200],\n",
      "        [ 603.9200,  176.2500,  622.7500,  191.5800],\n",
      "        [1035.6000,  236.4800, 1240.0000,  373.0000]]),\n",
      "  'labels': tensor([0, 0, 0, 2, 0, 1, 8, 8, 8])},\n",
      " {'boxes': tensor([[ 638.1900,  176.1000,  666.1500,  199.2200],\n",
      "        [ 689.1000,  183.0300,  775.3000,  244.7700],\n",
      "        [ 852.7100,  187.0600, 1241.0000,  374.0000],\n",
      "        [ 705.4900,  175.2700,  743.1100,  201.0800],\n",
      "        [ 759.3400,  176.7500,  831.4700,  216.3600],\n",
      "        [ 944.4400,    0.0000, 1241.0000,  334.1400],\n",
      "        [ 550.1900,  165.6500,  559.6100,  177.1500],\n",
      "        [ 614.7700,  168.7700,  628.3500,  182.3500]]),\n",
      "  'labels': tensor([0, 0, 0, 0, 0, 2, 8, 8])},\n",
      " {'boxes': tensor([[716.0000, 143.8300, 898.5300, 263.8300],\n",
      "        [302.8400, 189.5900, 439.1100, 282.7600],\n",
      "        [424.4500, 184.9700, 489.7900, 231.1500],\n",
      "        [455.0600, 173.0900, 507.9600, 217.7400],\n",
      "        [580.6700, 171.0700, 607.9800, 195.9800],\n",
      "        [525.7300, 151.4200, 559.6600, 191.1700],\n",
      "        [565.8400, 171.4800, 579.7200, 193.7700]]),\n",
      "  'labels': tensor([0, 0, 0, 0, 0, 8, 8])},\n",
      " {'boxes': tensor([[ 828.7800,  178.3500, 1208.2600,  374.0000],\n",
      "        [ 444.5000,  161.4700,  548.4700,  258.6500],\n",
      "        [ 833.7900,  182.3500,  920.1000,  353.9000],\n",
      "        [   0.0000,  209.1500,  372.3700,  374.0000],\n",
      "        [ 714.9400,  183.7000,  803.0600,  240.7800],\n",
      "        [ 757.4800,  181.7600,  896.5300,  273.6000],\n",
      "        [ 558.2300,  173.8900,  590.9400,  204.8000],\n",
      "        [ 494.1800,  178.2700,  563.2300,  233.4700],\n",
      "        [ 707.3900,  179.3500,  768.0900,  226.2500],\n",
      "        [ 691.0300,  178.2600,  738.9900,  214.0800],\n",
      "        [ 679.5300,  176.8300,  720.4300,  208.0600],\n",
      "        [ 594.9500,  168.8900,  680.6300,  198.2900]]),\n",
      "  'labels': tensor([0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 8])})\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "\n",
    "image, target = dataset[0]\n",
    "print(image)\n",
    "print(type(target))\n",
    "pp.pprint(target)\n",
    "\n",
    "\n",
    "print(\"data loader part\")\n",
    "for images, targets in data_loader:\n",
    "    print(len(images))\n",
    "    print(images[0].shape)\n",
    "    print(type(targets))\n",
    "    pp.pprint(targets)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on gpu if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "# Define the list of classes\n",
    "class_list = ['Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram', 'Misc', 'DontCare']\n",
    "\n",
    "num_classes = len(class_list) # one class (class 0) is dedicated to the \"background\"\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_object_detection_model(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "  optimizer,\n",
    "  step_size=3,\n",
    "  gamma=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [   0/1497]  eta: 3:23:07  lr: 0.000010  loss: 2.2444 (2.2444)  loss_classifier: 1.9724 (1.9724)  loss_box_reg: 0.0695 (0.0695)  loss_objectness: 0.0798 (0.0798)  loss_rpn_box_reg: 0.1226 (0.1226)  time: 8.1416  data: 0.0955\n",
      "Epoch: [0]  [  10/1497]  eta: 3:36:07  lr: 0.000060  loss: 2.2444 (2.2922)  loss_classifier: 1.9368 (1.9154)  loss_box_reg: 0.1781 (0.2183)  loss_objectness: 0.0725 (0.0861)  loss_rpn_box_reg: 0.0562 (0.0724)  time: 8.7207  data: 0.1029\n",
      "Epoch: [0]  [  20/1497]  eta: 3:27:12  lr: 0.000110  loss: 1.9197 (1.9275)  loss_classifier: 1.5829 (1.5444)  loss_box_reg: 0.1935 (0.2131)  loss_objectness: 0.0609 (0.0887)  loss_rpn_box_reg: 0.0583 (0.0813)  time: 8.4315  data: 0.0995\n",
      "Epoch: [0]  [  30/1497]  eta: 3:22:51  lr: 0.000160  loss: 1.0179 (1.5626)  loss_classifier: 0.5967 (1.1752)  loss_box_reg: 0.2019 (0.2138)  loss_objectness: 0.0845 (0.0958)  loss_rpn_box_reg: 0.0728 (0.0779)  time: 8.0635  data: 0.0976\n",
      "Epoch: [0]  [  40/1497]  eta: 3:18:49  lr: 0.000210  loss: 0.7477 (1.3583)  loss_classifier: 0.3568 (0.9668)  loss_box_reg: 0.2021 (0.2126)  loss_objectness: 0.1001 (0.0976)  loss_rpn_box_reg: 0.0570 (0.0813)  time: 7.9458  data: 0.0980\n",
      "Epoch: [0]  [  50/1497]  eta: 3:15:48  lr: 0.000260  loss: 0.6852 (1.2441)  loss_classifier: 0.3236 (0.8428)  loss_box_reg: 0.2021 (0.2145)  loss_objectness: 0.0883 (0.0968)  loss_rpn_box_reg: 0.0759 (0.0899)  time: 7.8436  data: 0.0971\n",
      "Epoch: [0]  [  60/1497]  eta: 3:13:25  lr: 0.000310  loss: 0.6430 (1.1436)  loss_classifier: 0.2841 (0.7509)  loss_box_reg: 0.1901 (0.2123)  loss_objectness: 0.0576 (0.0917)  loss_rpn_box_reg: 0.0759 (0.0887)  time: 7.8479  data: 0.0983\n",
      "Epoch: [0]  [  70/1497]  eta: 3:11:46  lr: 0.000360  loss: 0.6509 (1.0872)  loss_classifier: 0.2927 (0.6911)  loss_box_reg: 0.1986 (0.2189)  loss_objectness: 0.0683 (0.0909)  loss_rpn_box_reg: 0.0531 (0.0863)  time: 7.9225  data: 0.0955\n",
      "Epoch: [0]  [  80/1497]  eta: 3:09:46  lr: 0.000410  loss: 0.7348 (1.0518)  loss_classifier: 0.3226 (0.6471)  loss_box_reg: 0.2606 (0.2267)  loss_objectness: 0.0643 (0.0923)  loss_rpn_box_reg: 0.0580 (0.0857)  time: 7.9111  data: 0.0952\n",
      "Epoch: [0]  [  90/1497]  eta: 3:08:15  lr: 0.000460  loss: 0.7139 (1.0093)  loss_classifier: 0.3118 (0.6087)  loss_box_reg: 0.2689 (0.2296)  loss_objectness: 0.0527 (0.0885)  loss_rpn_box_reg: 0.0580 (0.0824)  time: 7.9029  data: 0.0967\n",
      "Epoch: [0]  [ 100/1497]  eta: 7:16:05  lr: 0.000509  loss: 0.6482 (0.9788)  loss_classifier: 0.3004 (0.5794)  loss_box_reg: 0.2689 (0.2349)  loss_objectness: 0.0367 (0.0843)  loss_rpn_box_reg: 0.0504 (0.0802)  time: 62.0424  data: 0.0959\n",
      "Epoch: [0]  [ 110/1497]  eta: 15:04:23  lr: 0.000559  loss: 0.6569 (0.9527)  loss_classifier: 0.2994 (0.5543)  loss_box_reg: 0.2392 (0.2379)  loss_objectness: 0.0367 (0.0825)  loss_rpn_box_reg: 0.0504 (0.0780)  time: 180.6030  data: 0.1016\n",
      "Epoch: [0]  [ 120/1497]  eta: 16:24:42  lr: 0.000609  loss: 0.6248 (0.9195)  loss_classifier: 0.2706 (0.5281)  loss_box_reg: 0.2392 (0.2358)  loss_objectness: 0.0355 (0.0795)  loss_rpn_box_reg: 0.0494 (0.0762)  time: 165.0007  data: 0.1026\n",
      "Epoch: [0]  [ 130/1497]  eta: 16:32:11  lr: 0.000659  loss: 0.5363 (0.8983)  loss_classifier: 0.2407 (0.5095)  loss_box_reg: 0.2418 (0.2387)  loss_objectness: 0.0323 (0.0767)  loss_rpn_box_reg: 0.0394 (0.0734)  time: 68.1146  data: 0.1018\n",
      "Epoch: [0]  [ 140/1497]  eta: 16:46:53  lr: 0.000709  loss: 0.6240 (0.8798)  loss_classifier: 0.2737 (0.4939)  loss_box_reg: 0.2655 (0.2418)  loss_objectness: 0.0316 (0.0735)  loss_rpn_box_reg: 0.0356 (0.0706)  time: 54.2803  data: 0.1088\n",
      "Epoch: [0]  [ 150/1497]  eta: 16:31:08  lr: 0.000759  loss: 0.6240 (0.8666)  loss_classifier: 0.2737 (0.4790)  loss_box_reg: 0.2636 (0.2434)  loss_objectness: 0.0327 (0.0736)  loss_rpn_box_reg: 0.0390 (0.0706)  time: 48.0762  data: 0.1068\n",
      "Epoch: [0]  [ 160/1497]  eta: 16:33:47  lr: 0.000809  loss: 0.5436 (0.8492)  loss_classifier: 0.2259 (0.4631)  loss_box_reg: 0.2164 (0.2417)  loss_objectness: 0.0399 (0.0729)  loss_rpn_box_reg: 0.0566 (0.0716)  time: 45.1457  data: 0.1027\n",
      "Epoch: [0]  [ 170/1497]  eta: 15:57:00  lr: 0.000859  loss: 0.5781 (0.8377)  loss_classifier: 0.2223 (0.4515)  loss_box_reg: 0.2188 (0.2427)  loss_objectness: 0.0566 (0.0724)  loss_rpn_box_reg: 0.0566 (0.0710)  time: 36.6430  data: 0.1055\n"
     ]
    }
   ],
   "source": [
    "# training for 5 epochs\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # training for one epoch\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for images, targets in data_loader:\n",
    "        labels = [t[\"labels\"] for t in targets]\n",
    "        if any(l.item() > 8 for l in torch.cat(labels)):\n",
    "            print(f\"Invalid label detected: {labels}\")\n",
    "        break  # Remove this after testing to run full training\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

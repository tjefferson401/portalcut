{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic python and ML Libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# We will be reading images using OpenCV\n",
    "import cv2\n",
    "\n",
    "# matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# torchvision libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as torchtrans\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# helper libraries\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.models.detection as detection\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "# Send train=True for training transforms and False for val/test transforms\n",
    "def get_transform():\n",
    "    transform = [transforms.ToTensor()]\n",
    "    return transforms.Compose(transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import KittiTorch\n",
    "\n",
    "# Assuming KittiTorch and utils are defined/imported correctly\n",
    "dataset = KittiTorch(root='../data', download=True, transform=get_transform())\n",
    "\n",
    "# Print initial dataset size\n",
    "print(\"Initial dataset size:\", len(dataset))\n",
    "\n",
    "# Seed and random permutation\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "\n",
    "# Calculate split sizes\n",
    "train_split = 0.8\n",
    "val_split = 0.1  # 10% for validation\n",
    "test_split = 0.1  # 10% for test\n",
    "\n",
    "# Calculate indices for splits\n",
    "train_size = int(len(dataset) * train_split)\n",
    "val_size = int(len(dataset) * val_split)\n",
    "test_size = len(dataset) - train_size - val_size  # To ensure full coverage\n",
    "\n",
    "# Split indices\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:]\n",
    "\n",
    "# Create dataset subsets\n",
    "dataset_train = torch.utils.data.Subset(dataset, train_indices)\n",
    "dataset_val = torch.utils.data.Subset(dataset, val_indices)\n",
    "dataset_test = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 8\n",
    "\n",
    "# Data loaders\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=utils.collate_fn,\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=utils.collate_fn,\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=utils.collate_fn,\n",
    ")\n",
    "\n",
    "# Print sizes of datasets to confirm splits\n",
    "print(\"Training set size:\", len(dataset_train))\n",
    "print(\"Validation set size:\", len(dataset_val))\n",
    "print(\"Testing set size:\", len(dataset_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {gpu_count}\")\n",
    "    \n",
    "    for i in range(gpu_count):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  - {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  - Memory Cached: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  - Memory Total: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"  - Compute Capability: {torch.cuda.get_device_capability(i)}\")\n",
    "        print(f\"  - Multiprocessors: {torch.cuda.get_device_properties(i).multi_processor_count}\")\n",
    "else:\n",
    "    print(\"No GPU is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_detection_model(num_classes):\n",
    "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "  # get number of input features for the classifier\n",
    "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "  # replace the pre-trained head with a new one\n",
    "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "\n",
    "#   for param in model.backbone.parameters():\n",
    "#         param.requires_grad = False\n",
    "        \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "\n",
    "image, target = dataset[0]\n",
    "print(image)\n",
    "print(type(target))\n",
    "pp.pprint(target)\n",
    "\n",
    "\n",
    "print(\"data loader part\")\n",
    "for images, targets in data_loader_train:\n",
    "    print(len(images))\n",
    "    print(images[0].shape)\n",
    "    print(type(targets))\n",
    "    pp.pprint(targets)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "lr = 10e-3\n",
    "\n",
    "# Define the list of classes\n",
    "class_list = ['Background', 'Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram', 'Misc', 'DontCare']\n",
    "num_classes = len(class_list)  # one class (class 0) is dedicated to the \"background\"\n",
    "\n",
    "# Assume get_object_detection_model is defined and returns a model instance\n",
    "model = get_object_detection_model(num_classes)\n",
    "\n",
    "# Move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# Construct an optimizer (using Adam here)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer = optim.Adam(params, lr=lr)  # Set a learning rate, modify as needed\n",
    "\n",
    "# sgd optmizer\n",
    "optimizer = optim.SGD(params, lr=lr, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Optionally, you can set other parameters like betas and eps\n",
    "# optimizer = optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Login to WandB (only needed if you haven't configured automatic login)\n",
    "wandb.login()\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "# Initialize a new WandB run\n",
    "wandb.init(project=\"portalcut\",\n",
    "            entity='231n-augmentation', \n",
    "            notes=\"2024-05-30-kitti-test1-fasterrcnn_resnet50_fpn_v2_scratch_50ep_v2\",\n",
    "            \n",
    "            config={\n",
    "                \"learning_rate\": lr,\n",
    "                \"epochs\": num_epochs,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"optimizer\": \"Adam\",\n",
    "            })\n",
    "\n",
    "\n",
    "config = wandb.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training for 5 epochs\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import utils\n",
    "\n",
    "data_loader = data_loader_train\n",
    "\n",
    "scaler = None  # Define the \"scaler\" variable\n",
    "\n",
    "model_save_path = './models/test1_v1.pth'\n",
    "print_freq = 10\n",
    "# Training loop\n",
    "\n",
    "# Assume we have an existing setup\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "        \n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log metrics to WandB\n",
    "        wandb.log({\n",
    "            **loss_dict,\n",
    "            \"epoch\": epoch,\n",
    "            \"loss\": loss_value,\n",
    "            \"learning_rate\": optimizer.param_groups[0][\"lr\"]\n",
    "        })\n",
    "        metric_logger.update(loss=losses, **loss_dict)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    # After each epoch\n",
    "    # lr_scheduler.step()\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "# wandb.log_artifact(model)\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# model.to_onnx()\n",
    "# wandb.save(\"model.onnx\")\n",
    "\n",
    "\n",
    "# Finish WandB run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {gpu_count}\")\n",
    "    \n",
    "    for i in range(gpu_count):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  - {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  - Memory Cached: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  - Memory Total: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"  - Compute Capability: {torch.cuda.get_device_capability(i)}\")\n",
    "        print(f\"  - Multiprocessors: {torch.cuda.get_device_properties(i).multi_processor_count}\")\n",
    "else:\n",
    "    print(\"No GPU is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = dataset_test[9][0]\n",
    "\n",
    "# input_tensor = image.unsqueeze(0).to('cuda')\n",
    "# predictions = model(input_tensor)\n",
    "\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_object_detection_model(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code that saves up the model from the internet and tests it\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#clear cache in cuda\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "image = dataset_test[9][0]\n",
    "# Load the model\n",
    "model.load_state_dict(torch.load('models/test1_v1.pth', map_location='cpu'))\n",
    "\n",
    "# Ensure your model is on the GPU\n",
    "model = model.to('cuda')\n",
    "\n",
    "# Move the input tensor to the GPU\n",
    "input_tensor = image.unsqueeze(0).to('cuda')\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient computation during inference\n",
    "with torch.no_grad():\n",
    "    predictions = model(input_tensor)\n",
    "\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_image_with_boxes(image, boxes, labels, label_names):\n",
    "    # Convert tensor image to numpy array\n",
    "    image = image.cpu().numpy().transpose((1, 2, 0))\n",
    "    # Scale the image's pixel values to [0, 255]\n",
    "    image = cv2.normalize(image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F).astype(np.uint8)\n",
    "    # Convert the image to CPU and NumPy format for plotting\n",
    "\n",
    "\n",
    "    # Define colors for different classes\n",
    "    colors = {\n",
    "        'Car': (255, 0, 0), 'Van': (0, 255, 0), 'Truck': (0, 0, 255),\n",
    "        'Pedestrian': (255, 255, 0), 'Person_sitting': (255, 0, 255), 'Cyclist': (0, 255, 255),\n",
    "        'Tram': (127, 127, 255), 'Misc': (255, 127, 127), \"Don'tCare\": (127, 127, 127)\n",
    "    }\n",
    "\n",
    "    # Draw boxes and labels\n",
    "    for box, label in zip(boxes, labels):\n",
    "        box = box.cpu().numpy().astype(int)\n",
    "        label = int(label.cpu())\n",
    "        box = box.astype(int)\n",
    "        label_text = label_names[label]\n",
    "        color = colors.get(label_text, (255, 255, 255))\n",
    "\n",
    "        # Draw rectangle\n",
    "        cv2.rectangle(image, (box[0], box[1]), (box[2], box[3]), color, 2)\n",
    "\n",
    "        # Put label\n",
    "        cv2.putText(image, label_text, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    # Display the image\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Define label names based on your dataset specifics\n",
    "label_names = ['Background', 'Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram', 'Misc', 'DontCare']\n",
    "\n",
    "# Sample call to the function\n",
    "\n",
    "target = predictions[0]\n",
    "# image = dataset[0][0]\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "# This assumes `image` is a tensor from the dataset, `boxes` is a tensor of bounding boxes, and `labels` is a tensor of label indices\n",
    "visualize_image_with_boxes(image, target['boxes'], target['labels'], label_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision.ops import box_iou\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the model\n",
    "\n",
    "# model = get_object_detection_model(10)\n",
    "# model.load_state_dict(torch.load(model_save_path))\n",
    "# model = model.to('cuda')\n",
    "# model.eval()\n",
    "\n",
    "# Define label names based on your dataset specifics\n",
    "label_names = ['Background', 'Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram', 'Misc', 'DontCare']\n",
    "\n",
    "def compute_iou(boxes1, boxes2):\n",
    "    if boxes1.numel() == 0 or boxes2.numel() == 0:\n",
    "        return torch.tensor([]).to(boxes1.device)\n",
    "    boxes1 = boxes1.to('cuda')\n",
    "    boxes2 = boxes2.to('cuda')\n",
    "    ious = box_iou(boxes1, boxes2)\n",
    "    return ious\n",
    "\n",
    "def evaluate_model(model, dataset, label_names, iou_threshold=0.5):\n",
    "    all_true_boxes = []\n",
    "    all_pred_boxes = []\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "\n",
    "    for idx in range(len(dataset)):\n",
    "        image, target = dataset[idx]\n",
    "        input_tensor = image.unsqueeze(0).to('cuda')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = model(input_tensor)[0]\n",
    "        \n",
    "        true_boxes = target['boxes'].to('cuda')\n",
    "        true_labels = target['labels'].to('cuda')\n",
    "        pred_boxes = predictions['boxes'].to('cuda')\n",
    "        pred_labels = predictions['labels'].to('cuda')\n",
    "\n",
    "        # Create masks for excluding 'Background' and 'DontCare' labels\n",
    "        exclude_labels = torch.tensor([label_names.index('Background'), label_names.index('DontCare')], device='cuda')\n",
    "        relevant_true_mask = ~(true_labels.unsqueeze(1) == exclude_labels).any(1)\n",
    "        relevant_pred_mask = ~(pred_labels.unsqueeze(1) == exclude_labels).any(1)\n",
    "\n",
    "        filtered_true_boxes = true_boxes[relevant_true_mask]\n",
    "        filtered_true_labels = true_labels[relevant_true_mask]\n",
    "        filtered_pred_boxes = pred_boxes[relevant_pred_mask]\n",
    "        filtered_pred_labels = pred_labels[relevant_pred_mask]\n",
    "\n",
    "        all_true_boxes.append(filtered_true_boxes)\n",
    "        all_pred_boxes.append(filtered_pred_boxes)\n",
    "        all_true_labels.append(filtered_true_labels)\n",
    "        all_pred_labels.append(filtered_pred_labels)\n",
    "\n",
    "\n",
    "    iou_scores = []\n",
    "    for true_boxes, pred_boxes in zip(all_true_boxes, all_pred_boxes):\n",
    "        iou_scores.append(compute_iou(true_boxes, pred_boxes))\n",
    "\n",
    "    mean_iou = torch.mean(torch.stack([torch.mean(iou) for iou in iou_scores if iou.numel() > 0]))\n",
    "    print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "\n",
    "    # Compute mAP (mean Average Precision)\n",
    "    aps = []\n",
    "    for i, label_name in enumerate(label_names):\n",
    "        if label_name == \"Background\" or label_name == \"DontCare\":\n",
    "            continue\n",
    "        \n",
    "        true_positives = []\n",
    "        false_positives = []\n",
    "        num_gt = 0\n",
    "\n",
    "        for true_boxes, true_labels, pred_boxes, pred_labels in zip(all_true_boxes, all_true_labels, all_pred_boxes, all_pred_labels):\n",
    "            gt_boxes = true_boxes[true_labels == i]\n",
    "            pred_boxes = pred_boxes[pred_labels == i]\n",
    "            num_gt += len(gt_boxes)\n",
    "\n",
    "            if len(pred_boxes) == 0:\n",
    "                continue\n",
    "            \n",
    "            ious = compute_iou(gt_boxes, pred_boxes)\n",
    "            if ious.numel() == 0:\n",
    "                continue\n",
    "            true_positive = ious.max(dim=0)[0] > iou_threshold\n",
    "            false_positive = ~true_positive\n",
    "\n",
    "            true_positives.extend(true_positive.cpu().numpy())\n",
    "            false_positives.extend(false_positive.cpu().numpy())\n",
    "        \n",
    "        tp_cumsum = np.cumsum(true_positives)\n",
    "        fp_cumsum = np.cumsum(false_positives)\n",
    "        precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-6)\n",
    "        recalls = tp_cumsum / (num_gt + 1e-6)\n",
    "\n",
    "        ap = np.trapz(precisions, recalls)\n",
    "        aps.append(ap)\n",
    "\n",
    "        print(f\"AP for {label_name}: {ap:.4f}\")\n",
    "\n",
    "    mAP = np.mean(aps)\n",
    "    print(f\"Mean Average Precision (mAP): {mAP:.4f}\")\n",
    "\n",
    "# Example usage\n",
    "evaluate_model(model, dataset_test, label_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision.ops import box_iou\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define label names based on your dataset specifics\n",
    "label_names = ['Background', 'Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram', 'Misc', 'DontCare']\n",
    "\n",
    "def compute_iou(boxes1, boxes2):\n",
    "    if boxes1.numel() == 0 or boxes2.numel() == 0:\n",
    "        return torch.tensor([]).to(boxes1.device)\n",
    "    boxes1 = boxes1.to('cuda')\n",
    "    boxes2 = boxes2.to('cuda')\n",
    "    ious = box_iou(boxes1, boxes2)\n",
    "    return ious\n",
    "\n",
    "def calculate_precision_recall_f1(pred_boxes, true_boxes, iou_threshold=0.5):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    for pred_box in pred_boxes:\n",
    "        if any(compute_iou(pred_box.unsqueeze(0), true_box.unsqueeze(0)) >= iou_threshold for true_box in true_boxes):\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "    fn = len(true_boxes) - tp\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def calculate_map(pred_boxes, true_boxes, iou_thresholds=np.linspace(0.5, 0.95, 10)):\n",
    "    aps = []\n",
    "    for iou_threshold in iou_thresholds:\n",
    "        precision, recall, _ = calculate_precision_recall_f1(pred_boxes, true_boxes, iou_threshold)\n",
    "        aps.append(precision * recall)\n",
    "\n",
    "    return np.mean(aps)\n",
    "\n",
    "def evaluate_model(model, dataset, label_names, iou_threshold=0.5):\n",
    "    all_true_boxes = []\n",
    "    all_pred_boxes = []\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "\n",
    "    model.eval()\n",
    "    for idx in range(len(dataset)):\n",
    "        image, target = dataset[idx]\n",
    "        input_tensor = image.unsqueeze(0).to('cuda')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = model(input_tensor)[0]\n",
    "        \n",
    "        true_boxes = target['boxes'].to('cuda')\n",
    "        true_labels = target['labels'].to('cuda')\n",
    "        pred_boxes = predictions['boxes'].to('cuda')\n",
    "        pred_labels = predictions['labels'].to('cuda')\n",
    "        \n",
    "        # Create masks for excluding 'Background' and 'DontCare' labels\n",
    "        exclude_labels = torch.tensor([label_names.index('Background'), label_names.index('DontCare')], device='cuda')\n",
    "        relevant_true_mask = ~(true_labels.unsqueeze(1) == exclude_labels).any(1)\n",
    "        relevant_pred_mask = ~(pred_labels.unsqueeze(1) == exclude_labels).any(1)\n",
    "\n",
    "        filtered_true_boxes = true_boxes[relevant_true_mask]\n",
    "        filtered_true_labels = true_labels[relevant_true_mask]\n",
    "        filtered_pred_boxes = pred_boxes[relevant_pred_mask]\n",
    "        filtered_pred_labels = pred_labels[relevant_pred_mask]\n",
    "\n",
    "        all_true_boxes.append(filtered_true_boxes)\n",
    "        all_pred_boxes.append(filtered_pred_boxes)\n",
    "        all_true_labels.append(filtered_true_labels)\n",
    "        all_pred_labels.append(filtered_pred_labels)\n",
    "\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou_scores = []\n",
    "    for true_boxes, pred_boxes in zip(all_true_boxes, all_pred_boxes):\n",
    "        iou_scores.append(compute_iou(true_boxes, pred_boxes))\n",
    "\n",
    "    mean_iou = torch.mean(torch.stack([torch.mean(iou) for iou in iou_scores if iou.numel() > 0]))\n",
    "    print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "\n",
    "    # Calculate Precision, Recall, F1-Score, and mAP\n",
    "    aps = []\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for i, label_name in enumerate(label_names):\n",
    "        if label_name == \"Background\" or label_name == \"DontCare\":\n",
    "            continue\n",
    "        \n",
    "        true_positives = []\n",
    "        false_positives = []\n",
    "        false_negatives = []\n",
    "        num_gt = 0\n",
    "\n",
    "        for true_boxes, true_labels, pred_boxes, pred_labels in zip(all_true_boxes, all_true_labels, all_pred_boxes, all_pred_labels):\n",
    "            gt_boxes = true_boxes[true_labels == i]\n",
    "            pred_boxes = pred_boxes[pred_labels == i]\n",
    "            num_gt += len(gt_boxes)\n",
    "\n",
    "            if len(pred_boxes) == 0:\n",
    "                false_negatives.append(len(gt_boxes))\n",
    "                continue\n",
    "            \n",
    "            ious = compute_iou(gt_boxes, pred_boxes)\n",
    "            if ious.numel() == 0:\n",
    "                false_negatives.append(len(gt_boxes))\n",
    "                continue\n",
    "\n",
    "            max_ious, _ = ious.max(dim=1)\n",
    "            detected = max_ious > iou_threshold\n",
    "            true_positives.extend(detected.cpu().numpy())\n",
    "            false_positives.extend(~detected.cpu().numpy())\n",
    "            false_negatives.append(len(gt_boxes) - detected.sum().item())\n",
    "            \n",
    "        tp_cumsum = np.cumsum(true_positives)\n",
    "        fp_cumsum = np.cumsum(false_positives)\n",
    "        fn_cumsum = np.cumsum(false_negatives)\n",
    "        \n",
    "        precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-6)\n",
    "        recalls = tp_cumsum / (num_gt + 1e-6)\n",
    "\n",
    "        ap = np.trapz(precisions, recalls)\n",
    "        aps.append(ap)\n",
    "\n",
    "        precision = tp_cumsum[-1] / (tp_cumsum[-1] + fp_cumsum[-1] + 1e-6)\n",
    "        recall = tp_cumsum[-1] / (num_gt + 1e-6)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1\n",
    "        total_samples += 1\n",
    "\n",
    "        print(f\"AP for {label_name}: {ap:.4f}\")\n",
    "\n",
    "    mAP = np.mean(aps)\n",
    "    avg_precision = total_precision / total_samples\n",
    "    avg_recall = total_recall / total_samples\n",
    "    avg_f1 = total_f1 / total_samples\n",
    "\n",
    "    # Ensure recall does not exceed 1\n",
    "    avg_recall = min(avg_recall, 1.0)\n",
    "\n",
    "    print(f\"Mean Average Precision (mAP): {mAP:.4f}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average F1-Score: {avg_f1:.4f}\")\n",
    "\n",
    "    return mean_iou, avg_precision, avg_recall, avg_f1, mAP\n",
    "\n",
    "# Example usage\n",
    "# Assuming model is loaded and dataset_test is prepared\n",
    "mean_iou, avg_precision, avg_recall, avg_f1, mAP = evaluate_model(model, dataset_test, label_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {gpu_count}\")\n",
    "    \n",
    "    for i in range(gpu_count):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  - {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  - Memory Cached: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  - Memory Total: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"  - Compute Capability: {torch.cuda.get_device_capability(i)}\")\n",
    "        print(f\"  - Multiprocessors: {torch.cuda.get_device_properties(i).multi_processor_count}\")\n",
    "else:\n",
    "    print(\"No GPU is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader version\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision.ops import box_iou\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define label names based on your dataset specifics\n",
    "label_names = ['Background', 'Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram', 'Misc', 'DontCare']\n",
    "\n",
    "def compute_iou(boxes1, boxes2):\n",
    "    if boxes1.numel() == 0 or boxes2.numel() == 0:\n",
    "        return torch.tensor([]).to(boxes1.device)\n",
    "    boxes1 = boxes1.to('cuda')\n",
    "    boxes2 = boxes2.to('cuda')\n",
    "    ious = box_iou(boxes1, boxes2)\n",
    "    return ious\n",
    "\n",
    "def calculate_precision_recall_f1(pred_boxes, true_boxes, iou_threshold=0.5):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    for pred_box in pred_boxes:\n",
    "        if any(compute_iou(pred_box.unsqueeze(0), true_box.unsqueeze(0)) >= iou_threshold for true_box in true_boxes):\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "    fn = len(true_boxes) - tp\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def calculate_map(pred_boxes, true_boxes, iou_thresholds=np.linspace(0.5, 0.95, 10)):\n",
    "    aps = []\n",
    "    for iou_threshold in iou_thresholds:\n",
    "        precision, recall, _ = calculate_precision_recall_f1(pred_boxes, true_boxes, iou_threshold)\n",
    "        aps.append(precision * recall)\n",
    "\n",
    "    return np.mean(aps)\n",
    "\n",
    "def evaluate_model(model, dataloader, label_names, device='cuda', iou_threshold=0.5):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    model.to(device)\n",
    "\n",
    "    all_true_boxes = []\n",
    "    all_pred_boxes = []\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "\n",
    "    # Create masks for excluding 'Background' and 'DontCare' labels\n",
    "    exclude_labels = torch.tensor([label_names.index('Background'), label_names.index('DontCare')], device=device)\n",
    "    \n",
    "    for images, targets in dataloader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = model(images)\n",
    "        \n",
    "        for i, prediction in enumerate(predictions):\n",
    "            true_boxes = targets[i]['boxes'].to(device)\n",
    "            true_labels = targets[i]['labels'].to(device)\n",
    "            pred_boxes = prediction['boxes'].to(device)\n",
    "            pred_labels = prediction['labels'].to(device)\n",
    "\n",
    "            relevant_true_mask = ~(true_labels.unsqueeze(1) == exclude_labels).any(1)\n",
    "            relevant_pred_mask = ~(pred_labels.unsqueeze(1) == exclude_labels).any(1)\n",
    "\n",
    "            filtered_true_boxes = true_boxes[relevant_true_mask]\n",
    "            filtered_true_labels = true_labels[relevant_true_mask]\n",
    "            filtered_pred_boxes = pred_boxes[relevant_pred_mask]\n",
    "            filtered_pred_labels = pred_labels[relevant_pred_mask]\n",
    "\n",
    "            all_true_boxes.append(filtered_true_boxes)\n",
    "            all_pred_boxes.append(filtered_pred_boxes)\n",
    "            all_true_labels.append(filtered_true_labels)\n",
    "            all_pred_labels.append(filtered_pred_labels)\n",
    "\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou_scores = []\n",
    "    for true_boxes, pred_boxes in zip(all_true_boxes, all_pred_boxes):\n",
    "        iou_scores.append(compute_iou(true_boxes, pred_boxes))\n",
    "\n",
    "    mean_iou = torch.mean(torch.stack([torch.mean(iou) for iou in iou_scores if iou.numel() > 0]))\n",
    "    print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "\n",
    "    # Calculate Precision, Recall, F1-Score, and mAP\n",
    "    aps = []\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for i, label_name in enumerate(label_names):\n",
    "        if label_name == \"Background\" or label_name == \"DontCare\":\n",
    "            continue\n",
    "        \n",
    "        true_positives = []\n",
    "        false_positives = []\n",
    "        false_negatives = []\n",
    "        num_gt = 0\n",
    "\n",
    "        for true_boxes, true_labels, pred_boxes, pred_labels in zip(all_true_boxes, all_true_labels, all_pred_boxes, all_pred_labels):\n",
    "            gt_boxes = true_boxes[true_labels == i]\n",
    "            pred_boxes = pred_boxes[pred_labels == i]\n",
    "            num_gt += len(gt_boxes)\n",
    "\n",
    "            if len(pred_boxes) == 0:\n",
    "                false_negatives.append(len(gt_boxes))\n",
    "                continue\n",
    "            \n",
    "            ious = compute_iou(gt_boxes, pred_boxes)\n",
    "            if ious.numel() == 0:\n",
    "                false_negatives.append(len(gt_boxes))\n",
    "                continue\n",
    "\n",
    "            max_ious, _ = ious.max(dim=1)\n",
    "            detected = max_ious > iou_threshold\n",
    "            true_positives.extend(detected.cpu().numpy())\n",
    "            false_positives.extend(~detected.cpu().numpy())\n",
    "            false_negatives.append(len(gt_boxes) - detected.sum().item())\n",
    "            \n",
    "        tp_cumsum = np.cumsum(true_positives)\n",
    "        fp_cumsum = np.cumsum(false_positives)\n",
    "        fn_cumsum = np.cumsum(false_negatives)\n",
    "        \n",
    "        precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-6)\n",
    "        recalls = tp_cumsum / (num_gt + 1e-6)\n",
    "\n",
    "        ap = np.trapz(precisions, recalls)\n",
    "        aps.append(ap)\n",
    "\n",
    "        precision = tp_cumsum[-1] / (tp_cumsum[-1] + fp_cumsum[-1] + 1e-6)\n",
    "        recall = tp_cumsum[-1] / (num_gt + 1e-6)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1\n",
    "        total_samples += 1\n",
    "\n",
    "        print(f\"AP for {label_name}: {ap:.4f}\")\n",
    "\n",
    "    mAP = np.mean(aps)\n",
    "    avg_precision = total_precision / total_samples\n",
    "    avg_recall = total_recall / total_samples\n",
    "    avg_f1 = total_f1 / total_samples\n",
    "\n",
    "    # Ensure recall does not exceed 1\n",
    "    avg_recall = min(avg_recall, 1.0)\n",
    "\n",
    "    print(f\"Mean Average Precision (mAP): {mAP:.4f}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average F1-Score: {avg_f1:.4f}\")\n",
    "\n",
    "    return mean_iou, avg_precision, avg_recall, avg_f1, mAP\n",
    "\n",
    "# Example usage\n",
    "# Assuming model is loaded and dataset_test is prepared\n",
    "mean_iou, avg_precision, avg_recall, avg_f1, mAP = evaluate_model(model, data_loader_val, label_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "# Define label names based on your dataset specifics\n",
    "label_names = ['Background', 'Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram', 'Misc', 'DontCare']\n",
    "\n",
    "def compute_iou(boxes1, boxes2):\n",
    "    if boxes1.numel() == 0 or boxes2.numel() == 0:\n",
    "        return torch.tensor([]).to(boxes1.device)\n",
    "    return box_iou(boxes1, boxes2)\n",
    "\n",
    "def evaluate_model(model, dataset, label_names, iou_threshold=0.5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_true_boxes = []\n",
    "    all_pred_boxes = []\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "    aps = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "\n",
    "    for idx in range(len(dataset)):\n",
    "        image, target = dataset[idx]\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(image)[0]\n",
    "\n",
    "        all_true_boxes.append(target['boxes'].to(device))\n",
    "        all_true_labels.append(target['labels'].to(device))\n",
    "        all_pred_boxes.append(predictions['boxes'].to(device))\n",
    "        all_pred_labels.append(predictions['labels'].to(device))\n",
    "\n",
    "    iou_scores = [compute_iou(t_boxes, p_boxes) for t_boxes, p_boxes in zip(all_true_boxes, all_pred_boxes)]\n",
    "    mean_iou = torch.mean(torch.cat([torch.mean(iou) for iou in iou_scores if iou.numel() > 0]))\n",
    "\n",
    "    # Calculate Precision, Recall, F1-Score, and mAP\n",
    "    for i, label_name in enumerate(label_names):\n",
    "        if label_name in [\"Background\", \"DontCare\"]:\n",
    "            continue\n",
    "        true_positives = []\n",
    "        false_positives = []\n",
    "        num_gt = 0\n",
    "\n",
    "        for true_boxes, true_labels, pred_boxes, pred_labels in zip(all_true_boxes, all_true_labels, all_pred_boxes, all_pred_labels):\n",
    "            mask = true_labels == i\n",
    "            gt_boxes = true_boxes[mask]\n",
    "            mask = pred_labels == i\n",
    "            pred_boxes = pred_boxes[mask]\n",
    "            num_gt += gt_boxes.size(0)\n",
    "            \n",
    "            if gt_boxes.size(0) == 0 or pred_boxes.size(0) == 0:\n",
    "                continue\n",
    "\n",
    "            ious = compute_iou(gt_boxes, pred_boxes)\n",
    "            max_ious, _ = ious.max(dim=0)\n",
    "            true_positives.append(max_ious > iou_threshold)\n",
    "            false_positives.append(max_ious <= iou_threshold)\n",
    "\n",
    "        true_positives = torch.cat(true_positives) if true_positives else torch.tensor([])\n",
    "        false_positives = torch.cat(false_positives) if false_positives else torch.tensor([])\n",
    "        tp = true_positives.sum().item()\n",
    "        fp = false_positives.sum().item()\n",
    "        fn = num_gt - tp\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (num_gt) if num_gt > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        aps.append(precision * recall)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "\n",
    "        print(f\"AP for {label_name}: {precision * recall:.4f}\")\n",
    "\n",
    "    mAP = np.mean(aps)\n",
    "    avg_precision = np.mean(precision_list)\n",
    "    avg_recall = np.mean(recall_list)\n",
    "    avg_f1 = np.mean(f1_list)\n",
    "\n",
    "    print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "    print(f\"Mean Average Precision (mAP): {mAP:.4f}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average F1-Score: {avg_f1:.4f}\")\n",
    "\n",
    "    return mean_iou, avg_precision, avg_recall, avg_f1, mAP\n",
    "\n",
    "# Example usage\n",
    "# Assuming model is loaded and dataset_test is prepared\n",
    "mean_iou, avg_precision, avg_recall, avg_f1, mAP = evaluate_model(model, dataset_test, label_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright I like you code, however, will it work if I send in a dataloader in stead of the dataset, here is how I create dataloader\n",
    "\n",
    "def get_dataloaders(dataset):\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "\n",
    "    # Calculate split sizes, it should add up to 1\n",
    "    train_split = 0.8\n",
    "    val_split = 0.1  # 10% for validation\n",
    "    test_split = 0.1  # 10% for test\n",
    "\n",
    "    # Calculate indices for splits\n",
    "    train_size = int(len(dataset) * train_split)\n",
    "    val_size = int(len(dataset) * val_split)\n",
    "    test_size = len(dataset) - train_size - val_size  # To ensure full coverage\n",
    "\n",
    "    # Split indices\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:train_size + val_size]\n",
    "    test_indices = indices[train_size + val_size:]\n",
    "\n",
    "    # Create dataset subsets\n",
    "    dataset_train = Subset(dataset, train_indices)\n",
    "    dataset_val = Subset(dataset, val_indices)\n",
    "    dataset_test = Subset(dataset, test_indices)\n",
    "\n",
    "    # Define batch size\n",
    "    batch_size = 4\n",
    "\n",
    "    # Data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=utils.collate_fn,\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=utils.collate_fn,\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=utils.collate_fn,\n",
    "    )\n",
    "\n",
    "    # Print sizes of datasets to confirm splits\n",
    "    print(\"Training set size:\", len(dataset_train))\n",
    "    print(\"Validation set size:\", len(dataset_val))\n",
    "    print(\"Testing set size:\", len(dataset_test))\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
